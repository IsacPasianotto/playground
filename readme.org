 #+title: Playground
 #+author: [[https://github.com/IsacPasianotto/][Isac Pasianotto]]

Vagrant-based system to spawn a ready to use multi-node Kubernetes/Slurm VMs cluster.
Usable for testing purposes, learning, and development.

* 0. Prerequisites and dependencies

The only prerequisites are basic virtualization tools, in particular with ~Vagrant~ and it's dependencies. The virtualization is done using `libvirt` and `qemu-kvm` on Linux systems, but it should be possible (not tested) to use other providers like VirtualBox or VMware.

On RHEL-based system you can install them with:

#+begin_src sh
sudo dnf install -y $(sed -r '/^#/d' requirements.txt)
#+end_src

and enable the virtualization services:

#+begin_src sh
sudo systemctl enable --now libvirtd
sudo usermod -aG libvirt $(whoami)
vagrant plugin install vagrant-libvirt
#+end_src



Even if it's not strictly necessary, I highly recommend to install ~vagrant-scp~ plugin to easily copy files to/from the VMs.

#+begin_src sh
vagrant plugin install vagrant-scp
#+end_src


* 1. Set up the VMs

To start the VMs run:

#+begin_src sh
sudo virsh net-define scripts/kube-net.xml
sudo virsh net-start kube-net
vagrant up --provider=libvirt --no-parallel
#+end_src

Once the VMs are up and running, you can access them with:

#+begin_src sh
vagrant ssh kube-00
#+end_src

Note that `vagrant` take cares of the SSH keys and the IP addresses of the VMs, however, if you need to access the VMs directly (e.g. to use the ssh extension for VScode) , you can use the ~ssh~ keys available in the [[./ssh][ssh]] directory.

** 1.1 kubeconfig file

You can control the cluster using the ~kubectl~ command. To do that you don't need to ssh into the master node every time, but you can use the ~kubeconfig~ file available in the ~vagrant~ user home directory. To use it, you can copy it to your local machine with:

#+begin_src sh
vagrant scp kube-00:/home/vagrant/.kube/config ./playground_kubeconfig.yaml
#+end_src

and then set the ~KUBECONFIG~ environment variable to point to the file:

#+begin_src sh
export KUBECONFIG=$(pwd)/playground_kubeconfig.yaml
#+end_src


**Important**: once a CNI plugin is installed - you can check it using ~kubectl~ or ~k9s~ - you must restart all the nodes to apply the changes.


* 3. Use the virtual cluster in HPC-flavored mode

The cluster is also configured to run Slurm, a job scheduler and resource manager for HPC systems.
For example to request some resource:

#+begin_src
salloc --partition=prod --nodes=2 --tasks-per-node=1
#+end_src



* Cluster specificatoins

The VMs are defined in the [[./Vagrantfile][Vagrantfile]] and can be customized to your needs.

  * By default the cluster are composed by 1 master (~kube-00~) and 2 workers nodes.
  * The VMs are based on Fedora cloud images
  * ~k8s~ is used as Kuberentes cluster manager
  * The default user is ~vagrant~ with password ~vagrant~
  * In the home of ~vagrant~ user, there is mounted as shared folder among nodes the `shared` directory. 
  * Feel free to changes number of nodes and resources, but be aware that:
    - To run Kubernetes the minimum requirements are 2GB of RAM and 2 CPUs
    - You need to manually adjust the various config files edited during the deployment (have a look at [[./scripts][./scripts]] directory)
